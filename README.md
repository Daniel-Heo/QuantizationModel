# QuantizationModel
양자화와 지식 증류등으로 모델의 성능과 사이즈를 최적화한다.

# 적용 기술
모델의 크기를 줄이는 것은 연산 속도를 향상시키고, 메모리 사용량을 줄이며, 모델을 더 많은 환경에서 배포 가능하게 만드는 데 중요합니다. 모델 크기를 줄이는 방법에는 여러 가지가 있습니다:

- 가중치 프루닝(Weight Pruning): 모델에서 중요도가 낮은 가중치를 제거함으로써 모델의 크기를 줄입니다. 이 과정에서 중요하지 않은 연결을 제거하고, 중요한 연결만을 유지하여 모델의 크기와 복잡성을 줄입니다.

- 양자화(Quantization): 모델의 가중치와 활성화를 더 낮은 비트 정밀도로 표현함으로써 모델의 크기를 줄입니다. 예를 들어, 32비트 부동소수점 가중치를 8비트 정수로 변환할 수 있습니다. 이 방법은 모델의 크기를 상당히 줄이면서도 성능 손실을 최소화할 수 있습니다.

- 지식 증류(Knowledge Distillation): 크고 복잡한 모델(교사 모델)로부터 학습한 정보를 더 작은 모델(학생 모델)에 전달하는 과정입니다. 학생 모델은 교사 모델이 가진 지식을 바탕으로 학습하여, 비슷한 성능을 유지하면서도 모델의 크기를 줄일 수 있습니다.

- 모델 아키텍처의 최적화: 모델 설계 단계에서부터 효율적인 아키텍처를 선택하거나 개발하는 것입니다. 예를 들어, 컨볼루셔널 뉴럴 네트워크(CNN)에서는 깊이별 분리 합성곱(Depthwise Separable Convolution)과 같은 효율적인 연산을 사용하여 모델의 크기와 연산량을 줄일 수 있습니다.

- 파라미터 공유(Parameter Sharing): 모델 내에서 가중치 또는 파라미터를 여러 레이어에서 공유함으로써 모델의 크기를 줄입니다. 이 방법은 특히 순환 신경망(RNN)과 같은 모델에서 유용할 수 있습니다.

- 희소성(Sparsity)을 이용한 훈련: 모델을 훈련시킬 때부터 희소성을 적용하여, 대부분의 가중치가 0이 되도록 합니다. 이로써 실제로 연산에 참여하는 가중치의 수를 줄이고, 저장 공간을 절약할 수 있습니다.

모델 크기를 줄이는 방법을 적용할 때는 성능 저하를 최소화하는 것이 중요합니다. 실험과 평가를 통해 모델의 성능과 크기 사이의 최적의 균형을 찾는 것이 필요합니다.

# 개발 방법론
Rapid 

# 대상 모델 
Whisper large model, NHNDQ/nllb-finetuned-en2ko model

# 목표
사이즈는 1/2로 축소, 성능은 30% 이상 개선

# 다음 일정
현재 AI를 사용하지만 AI Dependency한 부분을 없애서 컨트롤이 가능한 데이터와 연산방식으로 과학습된 AI Model에서 데이터의 중복성을 제거하고 부족한 부분(가독성과 문학성)을 개선한다. 
이를 위해서 AI로 해당 데이터들을 자동으로 생성할 수 있는 도구를 만든다.